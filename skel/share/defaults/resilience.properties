#  -----------------------------------------------------------------------
#     Default values for resilience
#
#     Note that the resilience standalone cell will not start
#     unless the global property dcache.resilience.mode is set to
#     'standalone'
#  -----------------------------------------------------------------------
@DEFAULTS_HEADER@

#
#  ---- Whether the standalone service is actually enabled.
#
(immutable)resilience.enabled-when-standalone=true
(immutable)resilience.enabled-when-embedded=false
(immutable)resilience.enabled-when-off=false

(one-of?true|false|${resilience.enabled-when-${dcache.resilience.mode}})resilience.enabled=${resilience.enabled-when-${dcache.resilience.mode}}

# Cell name of resilience service
resilience.cell.name=Resilience

#  ---- Whether to export the resilience cell as a well known cell
#
#  This property controls whether the resilience cell is published as
#  a well known cell. Well known cells are addressable through their
#  cell name, while other cells are only addressable from other domains
#  using their fully qualified cell address.
(one-of?true|false)resilience.cell.export=true

# ---- Message topics to subscribe to.
#
#       CorruptFileTopic is sent by ChecksumScanner
#       PoolStatusTopic is sent by PoolManager
#       CacheLocationTopic is forwarded from PnfsManager
#       ResilienceTopic is sent by pool and by interaction with the
#           pool selection unit via the admin commands.
#
resilience.cell.subscribe=CorruptFileTopic,PoolStatusTopic,ResilienceTopic,CacheLocationTopic

#  ---- Thread queues.
#
#       There are four thread queues associated with resilience handling
#           (see below). The first two are generally faster-running threads.
#           The latter two are slower, with (3) executing the most
#           time-consuming operation.
#
#       The settings here have been tuned to handle throughput on the order of
#           600+ new file messages a second.  In general it makes sense
#           to allocate the same number of threads for (1) and (2), and to give
#           more threads to the file copy task queue.
#           See further below for more details.
#

#  ---- Thread queue used during the registration of a new operation.
#       Each thread makes a call to the database (requires a connection).
#
resilience.submit-threads=100

#  ---- Thread queue used when an operation becomes active (to verify parameters
#       and either prepare a copy task, or remove a copy from a pool).
#       Each thread makes a call to the database (requires a connection).
#
resilience.pnfs-op-threads=100

#  ---- Thread queue used to do the actual file migration.
#       There are no database calls on this thread.
#
#       A note on pnfsid operation throttling.
#
#       In order to guarantee that two operations on the same pnfsid are
#       not run concurrently, a queueing strategy is used.  This strategy also
#       throttles the total number of distinct pnfsid operations which can be
#       handled at a given time.  This number is determined from the
#       maximum of (file-copy, pnfs-op) threads, which in most cases will
#       be the file-copy value.
#
resilience.file-copy-threads=200

#  ---- Thread queue used for scanning chimera on pool state changes or
#       as part of a periodic check.  Requires a database connection,
#       which it holds onto for the life of the task being executed.
#       It also requires a second connection to check attributes.
#
#       A note on pool operation throttling.
#
#       A pool scan or processing of a pool status message can generate
#       thousands, even millions, of pnfsid tasks.  Allowing too many pool
#       operations to run simultaneously can potentially overload the system.
#       Lowering the number of available threads may be necessary
#       if the number of files per pool is on the order of a million or
#       greater (or, alternately, one may need to increase the memory of the
#       JVM for the Resilience service).
#
#       A note on the pnfs tables memory footprint.  Each entry occupies
#       128 bytes minimum (assuming 8-byte alignment) of primitives or
#       object references.  4 of these references are to objects of variable
#       size on the heap which are usually shared with other structures.
#       If we assume each entry could encompass 512 bytes, then
#       10 million entries would require about 5 GB.
#
resilience.pool-scan-threads=10

#  ---- Configuration for database connection pool
#
#  The database connection pool reuses connections between successive
#  database operations.  By reusing connections dCache doesn't suffer
#  the overhead of establishing new database connections for each
#  operation.
#
#  The options here determine how the resilience behaves as the
#  number of concurrent requests fluctuates.
#

#
#  ---- The maximum number of concurrent database connections.
#
#       The recommended minimum setting is
#                                       resilience.submit-threads
#                                     + resilience.pnfs-op-threads
#                                     + 2*resilience.pool-scan-threads.
#
#       Submit and pnfs threads require 1 database connection, and scan
#       threads need 2.
#
#       Since this service shares the chimera database with pnfsmanager,
#       be sure to adjust the postgresql.conf max connections upwards
#       to accommodate both.  Pnfsmanager runs well with about 100
#       connections.  Adding a separate resilience service means the
#       connections should be increased by at least the amount below.
#
resilience.db.connections.max = 220

#
#  ---- The minimum number of idle database connections.
#
resilience.db.connections.idle = 1

#
#  ---- Database related settings reserved for internal use.
#
(immutable)resilience.db.host=${chimera.db.host}
(immutable)resilience.db.name=${chimera.db.name}
(immutable)resilience.db.user=${chimera.db.user}
(immutable)resilience.db.password=${chimera.db.password}
(immutable)resilience.db.password.file=${chimera.db.password.file}
(immutable)resilience.db.dialect=${chimera.db.dialect}
(immutable)resilience.db.url=${chimera.db.url}
(immutable)resilience.db.schema.changelog=${chimera.db.schema.changelog}
resilience.db.schema.auto=${dcache.db.schema.auto}

#
#  ---- File-system-related properties.
#
resilience.plugins.storage-info-extractor=org.dcache.chimera.namespace.ChimeraOsmStorageInfoExtractor
(one-of?ONLINE|NEARLINE)resilience.default-access-latency=NEARLINE
(one-of?CUSTODIAL|REPLICA|OUTPUT)resilience.default-retention-policy=CUSTODIAL
resilience.enable.inherit-file-ownership = false
resilience.enable.full-path-permission-check=true
resilience.enable.acl = false
resilience.upload-directory=${dcache.upload-directory}

#  ---- Base directory where any resilience metadata is stored.  This
#       includes the checkpoint file.
#
resilience.home=@dcache.paths.resilience@

#  ---- When resilience handling discovers replicas with no
#       available copies, it will write out a list of the pnfsids
#       on a per-pool basis to this directory on the host where
#       the namespace is running.  The files will be
#       named <pool-name>-inaccessible-yyyy.MM.dd.HH.mm.ss.
#
#       Note that currently the administrator must intervene and
#       take care of migrating these files by hand.
#
resilience.inaccessible-files.directory=${resilience.home}

#  ---- This buffer is used when fetching pnfsids from the namespace database.
#
resilience.pnfs-fetch-size=1000

#  ---- Pnfs operation checking.
#
#       The maximum interval which can pass before a check of waiting/completed
#       pnfs operations is run (for an active system the interval will be shorter,
#       as checks are also done each time a running task terminates).
#
resilience.pnfs-scan.period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.pnfs-scan.period.unit=MINUTES

#  ---- Pool manager pool info refreshing.
#
#       Information concerning a pool's cost is cached for this amount of
#       time, after which, it is refreshed by a call to the pool manager's
#       pool monitor.  Pool cost is requested/refreshed lazily on a pool-by-pool
#       basis.
#
resilience.cache-expiry=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.cache-expiry.unit=MINUTES

#  ---- Size of buffer for displaying history of completed pnfs operations.
#
resilience.history.buffer-size=1000

#  ---- Pool Status message handling.
#
#       How long to wait between the reception of a pool status DOWN message
#       and actually launching a scan operation to check replicas on
#       that pool.
#
resilience.pool-down-grace-period=10
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.pool-down-grace-period.unit=MINUTES

#  ---- Pool Status message handling.
#
#       Whether to initiate a scan of the pool to check resilience when
#       a RESTART message is received.  Off by default.
#
resilience.scan-on-pool-restart=false

#  ---- Startup
#
#       When an entire dcache installation is brought on line at the same time,
#       many pool status messages are generated.  Depending on how the resilience
#       system is configured to react to these messages (see the message handling
#       properties above), it may be desirable to allow the system turbulence
#       to subside.  The properties below control a frequency meter, which
#       samples the message traffic once every interval.  If it has decreased to
#       0 during that period, it notifies the system that it can now begin
#       initialization.  Setting this property to 0 skips the delay sampling.
#
#       Note that with the default settings above, this strategy is not
#       necessary, as no scanning will take place for the restart events,
#       which normally follow the initial pool down notifications.
#
#       Should an interval > 0 be expressed here, any messages arriving at
#       the resilience endpoint will be queued and the backlog will be handled
#       after startup.
#
resilience.startup-delay-sample-interval=0
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.startup-delay-sample-interval.unit=SECONDS


#  ---- Periodic scanning (watchdog).
#
#       The following properties control the periodic scanning of the resilient
#       pools to check for replica consistency and initiate any copies or removes
#       that may be necessary in the case of inconsistent state.  The scan period
#       refers to the default amount of time between sweeps of the pools (absent
#       pool status change events). The scan window refers to the maximum amount
#       of time that can elapse since a pool was scanned after which another
#       scan of that specific pool will be initiated. Disabling the watchdog
#       means the system will only respond to actual pool state change events,
#       but will not automatically scan pools after the window period has elapsed.
#
(one-of?true|false)resilience.watchdog.enabled=true
resilience.pool-scan.period=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.pool-scan.period.unit=MINUTES
resilience.pool-scan.window=24
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.pool-scan.window.unit=HOURS

#  ---- Copy/migration target selection.
#
#       Strategy implementation used to select among available/eligible pools.
#
resilience.pool-selection-strategy=org.dcache.pool.migration.ProportionalPoolSelectionStrategy

#  ---- Retry management.
#
#       The following properties control the number of
#       times the resilience system is allowed to retry failed copies.
#
resilience.max-retries=2

#  ---- Checkpointing.
#
#       How often the pnfs operation table is to be saved to disk for
#       the purposes of recovery.
#
resilience.checkpoint-expiry=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.checkpoint-expiry.unit=MINUTES

#  ---- Endpoint for contacting pool manager; this is needed for periodic
#       refreshing of live pool information.
#
resilience.service.poolmanager=${dcache.service.poolmanager}

#  ---- Endpoint for contacting pin manager (passed on to migration task).
#
resilience.service.pinmanager=${dcache.service.pinmanager}

#  ---- How long to wait for a response from the poolmanager.
#
resilience.service.poolmanager.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.service.poolmanager.timeout.unit=MINUTES

#  ---- How long to wait for a response from a pool.
#
resilience.service.pool.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.service.pool.timeout.unit=MINUTES