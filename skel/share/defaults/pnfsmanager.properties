#  -----------------------------------------------------------------------
#     Default values for pnfsmanager
#  -----------------------------------------------------------------------
@DEFAULTS_HEADER@

# Cell name of pnfsmanager service
pnfsmanager.cell.name=PnfsManager

#  ---- Whether to export the pnfsmanager cell as a well known cell
#
#  This property controls whether the pnfsmanager cell is published as
#  a well known cell. Well known cells are addressable through their
#  cell name, while other cells are only addressable from other domains
#  using their fully qualified cell address.
(one-of?true|false)pnfsmanager.cell.export=true

# Message topics to subscribe to.
#
(immutable)pnfsmanager.cell.subscribe-when-true=CorruptFileTopic,PoolStatusTopic,ResilienceTopic
(immutable)pnfsmanager.cell.subscribe-when-false=

pnfsmanager.cell.subscribe=${pnfsmanager.cell.subscribe-when-${pnfsmanager.enable.resilience}}

#   -- replace with org.dcache.chimera.namespace.ChimeraEnstoreStorageInfoExtractor
#      if you are running an enstore HSM backend.
#
pnfsmanager.plugins.storage-info-extractor = org.dcache.chimera.namespace.ChimeraOsmStorageInfoExtractor

#  ---- Number of threads per thread group
#
#   Depending on how powerful your chimera server host is you may set
#   this to up to 50.
#
pnfsmanager.limits.threads-per-group = 12

#  ---- Number of thread groups
#
#   A PNFS tree may be split into multiple databases. Each database is
#   single threaded and hence accessing the same database from
#   multiple threads provides only a minor speed-up. To ensure good
#   load balancing when using multiple databases, the PnfsManager
#   supports thread groups. Any database is assigned to one and only
#   one thread group, thus databases assigned to different thread
#   groups are guaranteed not to block each other. Each thread group
#   will have ${pnfsmanager.limits.threads-per-group} threads.
#
#   For best performance isolation, set this to be equal the largest
#   database ID defined in PNFS. When increasing
#   pnfsmanager.limits.thread-groups, you may want to lower
#   pnfsmanager.limits.threads-per-group.
#
#   Notice that PNFS access is still subject to the number of threads
#   created in the PNFS daemon. If this number is lower than the
#   number of concurrent requests, then contention may still occur
#   even though multiple databases are used.
#
pnfsmanager.limits.thread-groups = 1

#  ---- Number of list threads
#
#   The PnfsManager uses dedicated threads for directory list
#   operations. This variable controls the number of threads to
#   use.
#
pnfsmanager.limits.list-threads = 2

#  ---- Max chunk size in list replies
#
#   To avoid out of memory errors when listing large directories,
#   PnfsManager breaks up directory listings in chunk of entries. This
#   setting controls the maximum number of directory entries in a
#   chunk.
#
pnfsmanager.limits.list-chunk-size = 100

#  ---- Threshold for when to log slow requests
#
#   Threshold in milliseconds for when to log slow requests. Requests
#   with a processing time larger than the threshold are logged. Set
#   to 0 to disable. This can also be enabled at runtime using the
#   'set log slow threshold' command.
#
pnfsmanager.limits.log-slow-threshold=0


#  ---- Maximum number of requests in a processing queue
#
#   PnfsManager maintains a request queue per processing thread. This
#   setting specifies the queue length at which point new requests
#   will be denied rather than enqueued for processing. Set to 0 for
#   unlimitted queues.
#
pnfsmanager.limits.queue-length = 0

#  ---- Inherit file ownership when creating files and directories
#
#   By default new files and directories receive will be owned by the
#   person who created the files and directories. The owner field will
#   be the UID of the creator and the group field will be the primary
#   GID of the creator.
#
#   If this flag is set to true, then both the owner and the group
#   field will inherit the values from the parent directory.
#
#   In either case, a door may override the values with values
#   provided by the user.
#
(one-of?true|false)pnfsmanager.enable.inherit-file-ownership = false

#  ---- Whether to verify lookup permissions for the entire path
#
#   For performance reasons dCache with PNFS only verified the lookup
#   permissions of the directory containing the file system entry
#   corresponding to the path. Ie only the lookup permissions for the
#   last parent directory of the path were enforced. For compatibility
#   reasons Chimera inherited these semantics.
#
#   When this option is set to true, Chimera will verify the lookup
#   permissions of all directories of a path.
#
(one-of?true|false)pnfsmanager.enable.full-path-permission-check = true

#  ---- Enabled ACL support
#
#   Set to true to enable ACL support.
#
(one-of?true|false)pnfsmanager.enable.acl = false

#  ---- Whether to expect a space manager
(one-of?true|false|${dcache.enable.space-reservation})pnfsmanager.enable.space-reservation = ${dcache.enable.space-reservation}

# Comma separated list of cell addresses to which to send notifications when a file is flushed.
pnfsmanager.destination.flush-notification = ${pnfsmanager.destination.flush-notification-when-space-reservation-is-${pnfsmanager.enable.space-reservation}}
(immutable)pnfsmanager.destination.flush-notification-when-space-reservation-is-true=${pnfsmanager.service.spacemanager}
(immutable)pnfsmanager.destination.flush-notification-when-space-reservation-is-false=

# Cell address to which to send cache location change notifications
pnfsmanager.destination.cache-notification = CacheLocationTopic

# Cell address of space manager
pnfsmanager.service.spacemanager = ${dcache.service.spacemanager}

#  ---- Default Access Latency and Retention Policy
#
#   These variables affect only newly created files.
#
#   Do not use OUTPUT.
#
(one-of?CUSTODIAL|REPLICA|OUTPUT)pnfsmanager.default-retention-policy = CUSTODIAL

(one-of?ONLINE|NEARLINE)pnfsmanager.default-access-latency = NEARLINE

#  ---- Upload directory
#
pnfsmanager.upload-directory=${dcache.upload-directory}

#  ---- Configuration for database connection pool
#
#  The database connection pool reuses connections between successive
#  database operations.  By reusing connections dCache doesn't suffer
#  the overhead of establishing new database connections for each
#  operation.
#
#  The options here determine how the pnfsmanager behaves as the
#  number of concurrent requests fluctuates.
#

#
#  The maximum number of concurrent database connections.
#
#  NOTE:  when running resilience embedded here, this number should
#         be increased. The recommended minimum setting would be
#
#               pnfsmanager.resilience.submit-threads
#             + pnfsmanager.resilience.pnfs-op-threads
#             + pnfsmanager.resilience.db.connections.max
#             + whatever maximum allowed for normal namespace settings
#               (default = 30)
#
#       Submit and pnfs threads require 1 database connection, and scan
#       threads need 2.
#
#       Be sure to adjust postgresql.conf max connections to allow
#       for the larger value here, plus the added pool scan
#       connections specified by pnfsmanager.resilience.db.connections.max.
#
pnfsmanager.db.connections.max = 30

#
#  The minimum number of idle database connections.
#
pnfsmanager.db.connections.idle = 1

#
# Database related settings reserved for internal use.
#
(immutable)pnfsmanager.db.host=${chimera.db.host}
(immutable)pnfsmanager.db.name=${chimera.db.name}
(immutable)pnfsmanager.db.user=${chimera.db.user}
(immutable)pnfsmanager.db.password=${chimera.db.password}
(immutable)pnfsmanager.db.password.file=${chimera.db.password.file}
(immutable)pnfsmanager.db.dialect=${chimera.db.dialect}
(immutable)pnfsmanager.db.url=${chimera.db.url}
(immutable)pnfsmanager.db.schema.changelog=${chimera.db.schema.changelog}
pnfsmanager.db.schema.auto=${dcache.db.schema.auto}

#
#  --- Last Access Time (atime) updates for files
#
#  This integer value controls whether and when dCache updates the last access
#  time of files on reading them.
#
#  Values <  0: atimes are never updated.
#  Values >= 0: The maximum absolute(!) difference (in seconds) between a file's
#               "new" atime and its curently stored one, where the atime is not
#               yet updated.
#               For example, when using a value of "4" and the old atime is (in
#               POSIX time) "1000000000", then atimes up to including
#               "1000000004" (but also down to "999999996") are not written;
#               "1000000005" or later (respectively "999999995" or earlier)
#               would be saved.
#
#  Updating the atimes less often (or not at all) may have performance benefits.
#
pnfsmanager.atime-gap=-1

##
#  ------------------------------ Resilience ---------------------------------
#  These properties are relevant only when running resilience embedded in the
#  pnfsmanager cell.
#  ---------------------------------------------------------------------------
##
(immutable)pnfsmanager.enable.resilience-when-embedded=true
(immutable)pnfsmanager.enable.resilience-when-standalone=false
(immutable)pnfsmanager.enable.resilience-when-off=false

(one-of?true|false|${pnfsmanager.enable.resilience-when-${dcache.resilience.mode}})pnfsmanager.enable.resilience=${pnfsmanager.enable.resilience-when-${dcache.resilience.mode}}

#  ---- Thread queues.
#
#       There are four thread queues associated with resilience handling
#           (see below). The first two are generally faster-running threads.
#           The latter two are slower, with (3) executing the most
#           time-consuming operation.
#
#       In general it makes sense
#           to allocate the same number of threads for (1) and (2), and to give
#           more threads to the file copy task queue.
#           See further below for more details.
#

#  ---- Thread queue used during the registration of a new operation.
#       Each thread makes a call to the database (requires a connection).
#
pnfsmanager.resilience.submit-threads=100

#  ---- Thread queue used when an operation becomes active (to verify parameters
#       and either prepare a copy task, or remove a copy from a pool).
#       Each thread makes a call to the database (requires a connection).
#
pnfsmanager.resilience.pnfs-op-threads=100

#  ---- Thread queue used to do the actual file migration.
#       There are no database calls on this thread.
#
#       A note on pnfsid operation throttling.
#
#       In order to guarantee that two operations on the same pnfsid are
#       not run concurrently, a queueing strategy is used.  This strategy also
#       throttles the total number of distinct pnfsid operations which can be
#       handled at a given time.  This number is determined from the
#       maximum of (file-copy, pnfs-op) threads, which in most cases will
#       be the file-copy value.
#
pnfsmanager.resilience.file-copy-threads=200

#  ---- Thread queue used for scanning chimera on pool state changes or
#       as part of a periodic check.  Requires a database connection,
#       which it holds onto for the life of the task being executed.
#       It also requires a second connection to check attributes.
#
#       A note on pool operation throttling.
#
#       A pool scan or processing of a pool status message can generate
#       thousands, even millions, of pnfsid tasks.  Allowing too many pool
#       operations to run simultaneously can potentially overload the system.
#       Lowering the number of available threads may be necessary
#       if the number of files per pool is on the order of a million or
#       greater (or, alternately, one may need to increase the memory of the
#       JVM for the Resilience service).
#
#       A note on the pnfs tables memory footprint.  Each entry occupies
#       128 bytes minimum (assuming 8-byte alignment) of primitives or
#       object references.  4 of these references are to objects of variable
#       size on the heap which are usually shared with other structures.
#       If we assume each entry could encompass 512 bytes, then
#       10 million entries would require about 5 GB.
#
pnfsmanager.resilience.pool-scan-threads=10

#  ---- Database connection pool dedicated solely to resilience.
#       These settings are used with in conjunction with the pool scan
#       thread pool, so the max should be at least the size of that pool.
#
pnfsmanager.resilience.db.connections.max=10
pnfsmanager.resilience.db.connections.idle=1

#  ---- Base directory where any resilience metadata is stored.  This
#       includes the checkpoint file.
#
pnfsmanager.resilience.home=@dcache.paths.resilience@

#  ---- When resilience handling discovers replicas with no
#       available copies, it will write out a list of the pnfsids
#       on a per-pool basis to this directory on the host where
#       the namespace is running.  The files will be
#       named <pool-name>-inaccessible-yyyy.MM.dd.HH.mm.ss.
#
#       Note that currently the administrator must intervene and
#       take care of migrating these files by hand.
#
pnfsmanager.resilience.inaccessible-files.directory=${pnfsmanager.resilience.home}

#  ---- This buffer is used when fetching pnfsids from the namespace database.
#
pnfsmanager.resilience.pnfs-fetch-size=1000

#  ---- Pnfs operation checking.
#
#       The maximum interval which can pass before a check of waiting/completed
#       pnfs operations is run (for an active system the interval will be shorter,
#       as checks are also done each time a running task terminates).
#
pnfsmanager.resilience.pnfs-scan.period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.resilience.pnfs-scan.period.unit=MINUTES

#  ---- Pool manager pool info refreshing.
#
#       Information concerning a pool's cost is cached for this amount of
#       time, after which, it is refreshed by a call to the pool manager's
#       pool monitor.  Pool cost is requested/refreshed lazily on a pool-by-pool
#       basis.
#
pnfsmanager.resilience.cache-expiry=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.resilience.cache-expiry.unit=MINUTES

#  ---- Size of buffer for displaying history of completed pnfs operations.
#
pnfsmanager.resilience.history.buffer-size=1000

#  ---- Pool Status message handling.
#
#       How long to wait between the reception of a pool status DOWN message
#       and actually launching a scan operation to check replicas on
#       that pool.
#
pnfsmanager.resilience.pool-down-grace-period=10
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.resilience.pool-down-grace-period.unit=MINUTES

#  ---- Pool Status message handling.
#
#       Whether to initiate a scan of the pool to check resilience when
#       a RESTART message is received.  Off by default.
#
pnfsmanager.resilience.scan-on-pool-restart=false

#  ---- Startup
#
#       When an entire dcache installation is brought on line at the same time,
#       many pool status messages are generated.  Depending on how the resilience
#       system is configured to react to these messages (see the message handling
#       properties above), it may be desirable to allow the system turbulence
#       to subside.  The properties below control a frequency meter, which
#       samples the message traffic once every interval.  If it has decreased to
#       0 during that period, it notifies the system that it can now begin
#       initialization.  Setting this property to 0 skips the delay sampling.
#
#       Note that with the default settings above, this strategy is not
#       necessary, as no scanning will take place for the restart events,
#       which normally follow the initial pool down notifications.
#
#       Should an interval > 0 be expressed here, any messages arriving at
#       the resilience endpoint will be queued and the backlog will be handled
#       after startup.
#
pnfsmanager.resilience.startup-delay-sample-interval=0
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.resilience.startup-delay-sample-interval.unit=SECONDS

#  ---- Periodic scanning (watchdog).
#
#       The following properties control the periodic scanning of the resilient
#       pools to check for replica consistency and initiate any copies or removes
#       that may be necessary in the case of inconsistent state.  The scan period
#       refers to the default amount of time between sweeps of the pools (absent
#       pool status change events). The scan window refers to the maximum amount
#       of time that can elapse since a pool was scanned after which another
#       scan of that specific pool will be initiated. Disabling the watchdog
#       means the system will only respond to actual pool state change events,
#       but will not automatically scan pools after the window period has elapsed.
#
(one-of?true|false)pnfsmanager.resilience.watchdog.enabled=true
pnfsmanager.resilience.pool-scan.period=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.resilience.pool-scan.period.unit=MINUTES
pnfsmanager.resilience.pool-scan.window=24
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.resilience.pool-scan.window.unit=HOURS

#  ---- Copy/migration target selection.
#
#       Strategy implementation used to select among available/eligible pools.
#
pnfsmanager.resilience.pool-selection-strategy=org.dcache.pool.migration.ProportionalPoolSelectionStrategy

#  ---- Retry management.
#
#       The following properties control the number of
#       times the resilience system is allowed to retry failed copies.
#
pnfsmanager.resilience.max-retries=2

#  ---- Checkpointing.
#
#       How often the pnfs operation table is to be saved to disk for
#       the purposes of recovery.
#
pnfsmanager.resilience.checkpoint-expiry=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.resilience.checkpoint-expiry.unit=MINUTES

#  ---- Endpoint for contacting pool manager; this is needed for periodic
#       refreshing of live pool information.
#
pnfsmanager.service.poolmanager=${dcache.service.poolmanager}

#  ---- Endpoint for contacting pin manager (passed on to migration task).
#
pnfsmanager.service.pinmanager=${dcache.service.pinmanager}

#  ---- How long to wait for a response from the poolmanager.
#
pnfsmanager.service.poolmanager.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.service.poolmanager.timeout.unit=MINUTES

#  ---- How long to wait for a response from a pool.
#
pnfsmanager.service.pool.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)pnfsmanager.service.pool.timeout.unit=MINUTES


(obsolete)pnfsmanager.enable.folding = No longer supported